---
title: 'DATA621 HW #1'
author: "Farhana Akther"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: '3'
    toc_float: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview:

In this homework assignment, you will explore, analyze and model a data set containing 
approximately 2200 records. Each record represents a professional baseball team from the years 1871 
to 2006 inclusive. Each record has the performance of the team for the given year, with all of the 
statistics adjusted to match the performance of a 162 game season.

Your objective is to build a 'multiple linear regression model' on the 'training data' to predict the 
number of wins for the team. You can only use the variables given to you (or variables that you 
derive from the variables provided). Below is a short description of the variables of interest in 
the data set:

![](moneyball.jpg)


## Loading libraries:

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(GGally)
library(psych)
library(reshape2)
library(car)
library(mice)
```

# Introduction: 

In this specific problem, two datasets—training and testing—are provided for the Baseball game. The objective of this study is to delve into the training dataset by examining its dimensions, conducting descriptive summary statistics, plotting various features against the target variable, and assessing the correlation between them. Subsequently, we need to preprocess our dataset to prepare it for model training, addressing any missing values and outliers. Once the data is prepared, we can proceed to create different models using the features that exhibit greater statistical significance. After training the model, we will utilize the testing dataset to predict the target variable. Therefore, without further ado, let's commence this session with data exploration.


# 1. Data Exploration:

In this first step, we're going to look closely at the training data set to understand it better before we start preparing or modeling. Exploring the data helps us figure out what it's all about. Since the dataset is about a baseball game, where there are often misunderstandings about what actually makes a team win, it's our job as data scientists to find out the real factors that affect how well a team does using past data. To do that, we need to really know the data we're working with. That's why exploring the data is so important for the whole modeling process. So, let's begin by loading the dataset into our markdown.


## Loading Data:

The datasets (training and evaluation) has been uploaded to a GitHub repository, from which it has been loaded into the markdown using the code chunk provided below. The rationale behind uploading it to GitHub is to maintain the reproducibility of the work.

```{r Data Importation}
mb_evaluation <- read.csv("https://raw.githubusercontent.com/FarhanaAkther23/DATA621/main/moneyball-evaluation-data.csv", header=TRUE, sep=",")
mb_training <- read.csv("https://raw.githubusercontent.com/FarhanaAkther23/DATA621/main/moneyball-training-data.csv", header=TRUE, sep=",")
```


### Data Dimension:

```{r}
dim(mb_training)
```
From above code we can see that the dimension of our data in the training data set consists of 2276 observation and 17 variables. 

### Data set structure

```{r}
str(mb_training)
```

Data structure also shows the number of observation 2276 as well as the 17 variables. We can also see that all the observations are integer and there are a lot of N/As that we will need to address. 

### Descriptive Summary Statistics:

```{r}
summary(mb_training)
```

As we can see that the table above gave us the summary for INDEX column too. we will remove this from our data set as we are not going to use this on our analysis. The code chunk below remove the INDEX column from our data set and we are left with 16 variable. 

```{r}
training <- mb_training[,-1]
dim(training)
```

### Plotting the Target Column:

```{r}
ggplot()+
  geom_histogram(data = training, mapping = aes(x=TARGET_WINS), bins = 50, color = "black", fill = "grey")+geom_vline(xintercept=mean(training$TARGET_WINS), color='red')+labs(x="Target Wins", y="Count",title ="Distribution of Target Wins")+theme_bw()
```

The distribution of TARGET_WINS appears to be fairly normal, with a mean of around 80.8 (indicated by the red vertical line), a minimum of 0, and a maximum of 146. However, the presence of a minimum value of 0 is concerning, as it is highly unusual for a team to have zero wins. This suggests that there may be some data inconsistencies that need to be addressed. It would be advisable to revisit the summary statistics for the TARGET_WINS column to validate the observations mentioned above.

```{r}
summary(training$TARGET_WINS)
```


```{r}
hist(training$TARGET_WINS) 
```
The above plot looks fairly normally distributed. 

## Relational Plots Between Target and Feature column:

As we know that there are at least 15 features in the data set and it will be impractical to plot the relational graph between the target column and each feature column so we will use 'ggpairs'  from 'GGally'  library to plot multiple feature on the same graphic. In the graph below our focus will be on the first row and first column. The first row tell us the correlation between TARGET_WINS and feature column and similarly the first column show the relational plot between the same columns

### Finding Correlation using ggpairs(): 

let's look at the relationships between all variables using the following command:


```{r warning=FALSE, message=FALSE}
ggpairs(training [,  c(1, 2:5)]) 
```


```{r warning=FALSE, message=FALSE}
ggpairs(training [,  c(1, 6:9)]) 
```

```{r warning=FALSE, message=FALSE}
ggpairs(training [,  c(1, 10:13)]) 
```

```{r warning=FALSE, message=FALSE}
ggpairs(training [,  c(1, 14:16)]) 
```

From above we can see that the diagonal density plots provide a smooth representation of the distribution of each variable, while the scatter plots and correlation coefficients in the lower and upper triangles give insights into the relationships between variables. Some of these variables are collinear (correlated) and adding more than one of these variables to the model would not add much value. 

### Missing Values:

Let’s check out the missing values in the columns. We can check the number of missing values in each column using the code below:

```{r}
(colSums(is.na(training)))
```

# 2. Data Preparation:

## Missing Values and Outliers:

As we saw in the data exploration section,our data does contains missing values and outliers that we need to consider. We will try to address both missing values and outliers accordingly.

### Removing Missing values:

Earlier, we noticed that certain feature columns, such as 'TEAM_BATTING_HBP', 'TEAM_BASERUN_CS', and 'TEAM_FIELDING_DP', contain a lot of  missing values. It would be reasonable to exclude these columns from our dataset rather than trying to replace the missing values. Doing so can help us avoid potential issues related to accuracy and bias in our analysis.

```{r}
training <- training[, !names(training) %in% c('TEAM_BATTING_HBP','TEAM_BASERUN_CS','TEAM_FIELDING_DP')]
dim(training)
```

### Impute Missing values:

Let’s check out the remaining features for missing values


```{r}
(colSums(is.na(training)))
```
From the data above, it's evident that the columns 'TEAM_BATTING_SO', 'TEAM_BASERUN_SB', and 'TEAM_PITCHING_SO' contain some missing values, totaling 102 instances, which roughly corresponds to 4.5% of the entire dataset. In addressing these missing values, we will opt to replace them with the 'median' of their respective columns. This choice is motivated by the skewness observed in the distribution, as medians are less influenced by skewness compared to other measures.

```{r}
training$TEAM_BATTING_SO[is.na(training$TEAM_BATTING_SO)] <- median(training$TEAM_BATTING_SO, na.rm = TRUE)
training$TEAM_BASERUN_SB[is.na(training$TEAM_BASERUN_SB)] <- median(training$TEAM_BASERUN_SB, na.rm = TRUE)
training$TEAM_PITCHING_SO[is.na(training$TEAM_PITCHING_SO)] <- median(training$TEAM_PITCHING_SO, na.rm = TRUE)
```


### Fixing Outliers:

Looking at the summary and the plots below we see that 'PITCHING_H', 'PITCHING_BB', 'PITCHING_SO', and 'FIELDING_E' are all skewed due to the outliers. We also have some fields with a few missing values. we can fix these is to pick any value that is 3 standard deviations above the mean and impute them as the median.

```{r}
training$TEAM_PITCHING_H[training$TEAM_PITCHING_H > 3*sd(training$TEAM_PITCHING_H)] <- median(training$TEAM_PITCHING_H)
training$TEAM_PITCHING_BB[training$TEAM_PITCHING_BB > 3*sd(training$TEAM_PITCHING_BB)] <- median(training$TEAM_PITCHING_BB)
training$TEAM_PITCHING_SO[training$TEAM_PITCHING_SO > 3*sd(training$TEAM_PITCHING_SO)] <- median(training$TEAM_PITCHING_SO)
training$TEAM_FIELDING_E[training$TEAM_FIELDING_E > 3*sd(training$TEAM_FIELDING_E)] <- median(training$TEAM_FIELDING_E)
```


### Distribution Check:

```{r}
ggplot(melt(training), aes(x=value)) + geom_histogram(color = 'black', fill = 'grey') + facet_wrap(~variable, scale='free') + labs(x='', y='Frequency')+theme_bw()
```


# 3. Building Models: 

We will start with a full model that predicts  the number of win based on all the factors. 

## Model 1(m1): 

```{r}
set.seed(123)
m1 <- lm(TARGET_WINS ~., training)
summary(m1)
```
The R-squared value of 0.27 means that approximately 27% of the variability in the 'TARGET_WINS' can be explained by the predictor variables included in our model. The Adjusted R-squared value of 0.2661 means that, after adjusting for the number of predictor variables, approximately 26.61% of the variability in the 'Score' is explained by the model. Adjusted R-squared is typically slightly lower than R-squared, especially when additional predictor variables are included in the model. The adjustment accounts for the fact that adding more variables may increase R-squared by chance.Also, we can see that 'TEAM_PITCHING_HR' has a very high p-value which means that it is not statistically significant and it will be in our best interest to remove it from our next model (m2).


##### Performance Evaluation of Full Multiple Linear Regression Model:  

```{r}
# Make predictions on the training dataset
train_predictions <- predict(m1, newdata = training)
```


```{r}
# Assess the performance on the training dataset
train_residuals <- training$TARGET_WINS  - train_predictions
train_rmse <- sqrt(mean(train_residuals^(2)))
cat("Multiple Regression - Root Mean Squared Error (RMSE) on Training Data:", train_rmse, "\n")
```
```{r}
# Make predictions on the training dataset
train_predictions <- predict(m1, newdata = training)
```

```{r}
# Assess the performance on the training dataset
train_residuals <- training$TARGET_WINS  - train_predictions
train_rmse <- sqrt(mean(train_residuals^(2)))
cat("Multiple Regression - Root Mean Squared Error (RMSE) on Training Data:", train_rmse, "\n")
```

## Backward Stepwise Regression or Backward Elimination:

Backward Stepwise Regression is an approach to regression analysis that initiates with a full (saturated) model, including all variables, and systematically removes variables at each step to create a reduced model that optimally explains the data. This technique, also referred to as Backward Elimination regression, is valuable for addressing issues like **multicollinearity** and **overfitting**. By eliminating variables, it helps in reducing the number of predictors, contributing to a more **parsimonious** and interpretable model. This 'stepwise' approach aids in enhancing the model's generalization and interpretability by focusing on the most influential variables while eliminating those with lesser impact.


## Model 2(m2):

```{r}
set.seed(132)
m2<- lm(TARGET_WINS~TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_PITCHING_H+TEAM_PITCHING_BB+TEAM_FIELDING_E, training)
summary(m2)
```
As we can that after removing the TEAM_PITCHING_HR from the model our r-squared has dropped down a  0.27 to 0.2661.  however,  features like 'TEAM_BATTING_SO' and 'TEAM_BATTING_2B' has lost their significance simultaneously, with 'p-values' above .05. At this point it would be a good idea to check colinierity using cor() and multicollinearity with VIF(0. 

# Checking for colinierity: 

```{r}  
cor(training)
```

# Checking for Multicollinearity with VIF(): 

When constructing a Linear Regression model, it's important to address the possibility of multicollinearity among variables. Multicollinearity occurs when independent variables are highly correlated with each other, diminishing the reliability of statistical inferences. This phenomenon undermines the uniqueness of information within the regression model. Therefore, it's necessary to identify and mitigate multicollinearity by eliminating correlated variables when developing a multiple regression model.

The Variance Inflation Factor (VIF) serves as a tool for detecting multicollinearity within a model. It quantifies the correlation and its strength between independent variables in a regression model. The interpretation of VIF values is as follows:

    VIF < 1: No correlation
    1 < VIF < 5: Moderate correlation
    VIF > 5: Severe correlation

To illustrate, this guide provides an example of calculating VIF for a dataset using R.

```{r}
#calculate the VIF for each predictor variable in the model(m1)
vif(m1)
```

# Visualizing VIF Values:

```{r}
# create vector of VIF values
vif_values <- vif(m1)

# increase margin size to accommodate longer variable names
par(mar = c(3, 8, 3, 2) + 2.5)

# create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "gray",
        names.arg = names(vif_values), las = 2)

# add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)

```

From the above VIF() and the VID plot, we find out that independent variables: 'TEAM_BATTING_HR' 'TEAM_BATTING_SO', 'TEAM_PITCHING_HR' and 'TEAM_PITCHING_SO' are highly correlated with each other. This can  reduce the reliability of statistical inferences and  undermines the uniqueness of information within our regression  model. Therefore, will eliminate these correlated variables in our 3rd multiple regression model as well as 'TEAM_BATTING_2B' as it has low to no significance in our model. 


```{r}
colnames(training)
```

## Model 3 (m3):

```{r}
set.seed(321)
m3<- lm(TARGET_WINS~TEAM_BATTING_H + TEAM_BATTING_3B + TEAM_BATTING_BB + TEAM_BASERUN_SB + TEAM_PITCHING_H + TEAM_PITCHING_BB + TEAM_FIELDING_E, training)
summary(m3)
```

```{r}
vif(m3)
```


```{r}
# Make predictions on the training dataset
train_predictions2 <- predict(m3, newdata = training)
```

```{r}
# Assess the performance on the training dataset
train_residuals2 <- training$TARGET_WINS  - train_predictions2
train_rmse2 <- sqrt(mean(train_residuals2^(2)))
cat("Multiple Regression - Root Mean Squared Error (RMSE) on Training Data:", train_rmse2, "\n")
```
- Form above we can see the as expected, the RMSE increases 0.161 as we have eliminated almost 50% of predictor variables however this is not a significant difference as well compared to the previous model.

##### Residuals:

We can use residual plots to evaluate whether the conditions of least squares regression are reasonable. 

**To assess whether the linear model is reliable, we need to check for:** 

(1) linearity,
(2) nearly normal residuals, and
(3) constant variability

```{r}
par(mfrow=c(2,2))
plot(m3)
hist(resid(m3), main="Histogram of Residuals")
```

1. **Residuals vs. Fitted Values Plot:** No discernible pattern in the residuals.

2. **Normal Q-Q Plot:** The residuals follow a roughly straight line. Deviations from a straight line are visible on the top right of the tail ends suggesting slight departures from normality. 

3. **Scale-Location Plot:** This plot is used to check the assumption of equal variance (also called “homoscedasticity”) among the residuals in our regression model. The red line is roughly horizontal across the plot, the assumption of equal variance is not violated.

4. **Residuals vs. Leverage Plot:** No influential data points or outliers that significantly impact the regression results. some observation lies close to the border of Cook’s distance, but they do not fall outside of the dashed line. This means there aren’t any overly influential points in our dataset.

5. **Histogram of Residuals:** From the Histogram, the residuals show unimodal somewhat normal distribution. No apparent outliers. 


From the above plots, we can see that the data show positive nearly normal linearity and from the residual plots, we can observe that there seems to be constant variability

# Model Selection and Prediction:

### Selecting Models:

We are going with model 3 for prediction since all the feature all statistically significant and there is no weak correlation between the target and features. We have chosen the Multiple Linear Regression (m3) as the ideal model in our analysis as it consists almost 50% fewer predictor variables, less potential for overfitting the model as well as simple with effective R squared and RMSE. Additionally, we use less computational resources when building simpler models. 

Below are some of the main features of model 3 (m3)

```{r}
sum_m3 <- summary(m3)
RSS <- c(crossprod(m3$residuals))
MSE <- RSS/length(m3$residuals)
print(paste0("Mean Squared Error: ", MSE)) #a measure of the average squared difference between the predicted and observed values
```

```{r} 
print(paste0("Root MSE: ", sqrt(MSE))) #a measure of the average squared difference between the predicted and observed values.
```

```{r}
print(paste0("Adjusted R-squared: ", sum_m3$adj.r.squared))# measure of the proportion of variance in the dependent variable that is explained by the independent variables, adjusted for the number of predictors in the model.
```

```{r}
print(paste0("F-statistic: ",sum_m3$fstatistic[1])) # a measure of the overall significance of the regression model
```

Below is the distribution of the residuals which show no pattern across the horizontal line as directly displays the individual residuals compared to our Residuals vs Fitted plot..

```{r}
plot(resid(m3))
abline(h=0, col=2)
```

# Predicting the Data:

Now we have selected our model so we can examine our evaluation data set to predict the TARGET_WINS using m3 model. 

```{r}
dim(mb_evaluation)
```

```{r}
str(mb_evaluation)
```

```{r}
summary(mb_evaluation)
```

## Removing Unwanted columns:

Let’s remove the columns that we do not need for this analysis:
```{r}
evaluation <- mb_evaluation[, !names(mb_evaluation) %in% c('INDEX','TEAM_BATTING_HBP','TEAM_BASERUN_CS','TEAM_FIELDING_DP')]
```

```{r}
dim(evaluation)
```

```{r}
str(evaluation)
```


```{r}
colSums(is.na(evaluation))
```

For Imputation we have used MICE package: 

```{r}
evaluation <- mice(evaluation, m=5, maxit = 5, method = 'pmm')
evaluation <- complete(evaluation)
```


## Fixing Outliers:

```{r}
evaluation$TEAM_PITCHING_H[evaluation$TEAM_PITCHING_H > 3*sd(evaluation$TEAM_PITCHING_H)] <- median(evaluation$TEAM_PITCHING_H)
evaluation$TEAM_PITCHING_BB[evaluation$TEAM_PITCHING_BB > 3*sd(evaluation$TEAM_PITCHING_BB)] <- median(evaluation$TEAM_PITCHING_BB)
evaluation$TEAM_PITCHING_SO[evaluation$TEAM_PITCHING_SO > 3*sd(evaluation$TEAM_PITCHING_SO)] <- median(evaluation$TEAM_PITCHING_SO)
evaluation$TEAM_FIELDING_E[evaluation$TEAM_FIELDING_E > 3*sd(evaluation$TEAM_FIELDING_E)] <- median(evaluation$TEAM_FIELDING_E)
```

#Final Prediction:

The following code predicts the TARGET_WINS using the evaluation data set:

```{r}
final <- predict(m3, newdata = evaluation, interval="prediction")
(head(final,3))
```

The above output consists of three rows, each representing a prediction along with its corresponding prediction interval. To interpret each column:

  'fit': This column contains the point estimate or the predicted value for the target variable TARGET_WINS.

  'lwr': This column represents the lower bound of the prediction interval, indicating the lower limit within which the true value of TARGET_WINS is expected to fall with a certain confidence level.

 'upr': This column denotes the upper bound of the prediction interval, indicating the upper limit within which the true value of TARGET_WINS is expected to fall with a certain confidence level.

For example, in the first row, the point estimate or predicted value for TARGET_WINS is approximately 66.76482, the lower bound of the prediction interval is approximately 39.97970 and the upper bound of the prediction interval is approximately 93.54994.

This means that with a certain level of confidence, the true value of TARGET_WINS is expected to fall between approximately 39.97970 and 93.54994, with the point estimate being approximately 66.76482. Similar interpretations can be made for the other row


# Conclusion: 

In this assignment, a comprehensive analysis was conducted on a moderately large training dataset to develop a predictive model for the target variable. The process commenced with the loading of the dataset into the environment, followed by an exploratory phase where descriptive summary statistics and various distribution and relational plots were utilized to gain insights into the data's characteristics.

Subsequently, the data preparation stage involved wrangling the training data, which included addressing missing values and outliers to ensure the data set's quality and integrity. With the refined training dataset in hand, the next step involved model creation, where three distinct models were constructed based on the statistical significance of the features employed.

Following model creation, a rigorous evaluation process ensued to select the most robust model, considering the statistical significance of its features. Once the optimal model was identified, it was applied to the evaluation dataset to generate predictions for the target variable, thereby concluding the analysis.

**source:**

https://www.projectpro.io/recipes/find-vif-on-data-r
https://www.rdocumentation.org/packages/GGally/versions/2.2.0/topics/ggpairs 

https://www.rdocumentation.org/packages/regclass/versions/1.6/topics/VIF
https://www.statology.org/variance-inflation-factor-r/






# From Peter Gatica
#####

```{r}
coef(m1)
```


```{r}
ggplot(training,aes(x=TEAM_BATTING_H,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE, color='red')
```

```{r}
ggplot(training,aes(x=TEAM_BATTING_HR,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE, color='red')
```

```{r}
ggplot(training,aes(x=TEAM_BATTING_BB,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE,, color='red')
```

```{r}
ggplot(training,aes(x=TEAM_PITCHING_H,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE, color='red')
```

```{r}
ggplot(training,aes(x=TEAM_PITCHING_H,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE, color='red')
```

```{r}
ggplot(training,aes(x=TEAM_FIELDING_E,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE, color='red')
```

```{r}
ggplot(training,aes(x=TEAM_BATTING_SO,y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = 'lm', se = FALSE, color='red')
```



